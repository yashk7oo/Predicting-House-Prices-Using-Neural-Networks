{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\yashk\\Desktop\\housepricedata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8450,     7,     5, ...,     0,   548,     1],\n",
       "       [ 9600,     6,     8, ...,     1,   460,     1],\n",
       "       [11250,     7,     5, ...,     1,   608,     1],\n",
       "       ...,\n",
       "       [ 9042,     7,     9, ...,     2,   252,     1],\n",
       "       [ 9717,     5,     6, ...,     0,   240,     0],\n",
       "       [ 9937,     5,     6, ...,     0,   276,     0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dataset[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashk\\Anaconda3\\envs\\First Environment\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "X_scale=min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yashk\\Anaconda3\\envs\\First Environment\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(10,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yashk\\Anaconda3\\envs\\First Environment\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1022 samples, validate on 219 samples\n",
      "Epoch 1/700\n",
      "1022/1022 [==============================] - 1s 656us/step - loss: 0.6829 - acc: 0.5313 - val_loss: 0.6756 - val_acc: 0.5936\n",
      "Epoch 2/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6800 - acc: 0.5509 - val_loss: 0.6734 - val_acc: 0.6347\n",
      "Epoch 3/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.6765 - acc: 0.6067 - val_loss: 0.6716 - val_acc: 0.6986\n",
      "Epoch 4/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.6736 - acc: 0.6517 - val_loss: 0.6701 - val_acc: 0.7123\n",
      "Epoch 5/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.6710 - acc: 0.6859 - val_loss: 0.6687 - val_acc: 0.7215\n",
      "Epoch 6/700\n",
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.6688 - acc: 0.7074 - val_loss: 0.6674 - val_acc: 0.7215\n",
      "Epoch 7/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6667 - acc: 0.7260 - val_loss: 0.6663 - val_acc: 0.7534\n",
      "Epoch 8/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.6642 - acc: 0.7505 - val_loss: 0.6651 - val_acc: 0.7580\n",
      "Epoch 9/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6625 - acc: 0.7466 - val_loss: 0.6641 - val_acc: 0.7443\n",
      "Epoch 10/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6607 - acc: 0.7564 - val_loss: 0.6632 - val_acc: 0.7123\n",
      "Epoch 11/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.6585 - acc: 0.7456 - val_loss: 0.6620 - val_acc: 0.7260\n",
      "Epoch 12/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.6570 - acc: 0.7446 - val_loss: 0.6610 - val_acc: 0.7123\n",
      "Epoch 13/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.6552 - acc: 0.7485 - val_loss: 0.6600 - val_acc: 0.7032\n",
      "Epoch 14/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6537 - acc: 0.7417 - val_loss: 0.6592 - val_acc: 0.6986\n",
      "Epoch 15/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6520 - acc: 0.7309 - val_loss: 0.6579 - val_acc: 0.6986\n",
      "Epoch 16/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.6506 - acc: 0.7339 - val_loss: 0.6564 - val_acc: 0.7123\n",
      "Epoch 17/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.6494 - acc: 0.7436 - val_loss: 0.6556 - val_acc: 0.7123\n",
      "Epoch 18/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6479 - acc: 0.7378 - val_loss: 0.6546 - val_acc: 0.7078\n",
      "Epoch 19/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6463 - acc: 0.7378 - val_loss: 0.6538 - val_acc: 0.7032\n",
      "Epoch 20/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.6448 - acc: 0.7319 - val_loss: 0.6528 - val_acc: 0.6986\n",
      "Epoch 21/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.6434 - acc: 0.7290 - val_loss: 0.6515 - val_acc: 0.6986\n",
      "Epoch 22/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.6420 - acc: 0.7348 - val_loss: 0.6506 - val_acc: 0.6986\n",
      "Epoch 23/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.6405 - acc: 0.7309 - val_loss: 0.6498 - val_acc: 0.6895\n",
      "Epoch 24/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6391 - acc: 0.7241 - val_loss: 0.6484 - val_acc: 0.6986\n",
      "Epoch 25/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6378 - acc: 0.7280 - val_loss: 0.6471 - val_acc: 0.6986\n",
      "Epoch 26/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.6364 - acc: 0.7309 - val_loss: 0.6459 - val_acc: 0.6986\n",
      "Epoch 27/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.6350 - acc: 0.7348 - val_loss: 0.6450 - val_acc: 0.6941\n",
      "Epoch 28/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.6336 - acc: 0.7319 - val_loss: 0.6436 - val_acc: 0.7032\n",
      "Epoch 29/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6322 - acc: 0.7387 - val_loss: 0.6424 - val_acc: 0.7032\n",
      "Epoch 30/700\n",
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.6308 - acc: 0.7358 - val_loss: 0.6413 - val_acc: 0.6986\n",
      "Epoch 31/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.6295 - acc: 0.7397 - val_loss: 0.6401 - val_acc: 0.7078\n",
      "Epoch 32/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.6281 - acc: 0.7368 - val_loss: 0.6389 - val_acc: 0.7078\n",
      "Epoch 33/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.6267 - acc: 0.7436 - val_loss: 0.6373 - val_acc: 0.7215\n",
      "Epoch 34/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6253 - acc: 0.7456 - val_loss: 0.6364 - val_acc: 0.7169\n",
      "Epoch 35/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6240 - acc: 0.7436 - val_loss: 0.6349 - val_acc: 0.7215\n",
      "Epoch 36/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.6226 - acc: 0.7485 - val_loss: 0.6332 - val_acc: 0.7397\n",
      "Epoch 37/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.6211 - acc: 0.7515 - val_loss: 0.6317 - val_acc: 0.7397\n",
      "Epoch 38/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.6197 - acc: 0.7583 - val_loss: 0.6304 - val_acc: 0.7489\n",
      "Epoch 39/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.6183 - acc: 0.7613 - val_loss: 0.6291 - val_acc: 0.7489\n",
      "Epoch 40/700\n",
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.6169 - acc: 0.7642 - val_loss: 0.6277 - val_acc: 0.7489\n",
      "Epoch 41/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.6155 - acc: 0.7652 - val_loss: 0.6261 - val_acc: 0.7489\n",
      "Epoch 42/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.6140 - acc: 0.7691 - val_loss: 0.6250 - val_acc: 0.7489\n",
      "Epoch 43/700\n",
      "1022/1022 [==============================] - 0s 37us/step - loss: 0.6126 - acc: 0.7671 - val_loss: 0.6233 - val_acc: 0.7489\n",
      "Epoch 44/700\n",
      "1022/1022 [==============================] - 0s 39us/step - loss: 0.6112 - acc: 0.7710 - val_loss: 0.6221 - val_acc: 0.7489\n",
      "Epoch 45/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.6097 - acc: 0.7720 - val_loss: 0.6209 - val_acc: 0.7489\n",
      "Epoch 46/700\n",
      "1022/1022 [==============================] - 0s 38us/step - loss: 0.6083 - acc: 0.7710 - val_loss: 0.6195 - val_acc: 0.7489\n",
      "Epoch 47/700\n",
      "1022/1022 [==============================] - 0s 41us/step - loss: 0.6069 - acc: 0.7740 - val_loss: 0.6185 - val_acc: 0.7489\n",
      "Epoch 48/700\n",
      "1022/1022 [==============================] - 0s 57us/step - loss: 0.6053 - acc: 0.7730 - val_loss: 0.6165 - val_acc: 0.7534\n",
      "Epoch 49/700\n",
      "1022/1022 [==============================] - 0s 42us/step - loss: 0.6038 - acc: 0.7798 - val_loss: 0.6150 - val_acc: 0.7580\n",
      "Epoch 50/700\n",
      "1022/1022 [==============================] - 0s 42us/step - loss: 0.6022 - acc: 0.7808 - val_loss: 0.6135 - val_acc: 0.7580\n",
      "Epoch 51/700\n",
      "1022/1022 [==============================] - 0s 36us/step - loss: 0.6009 - acc: 0.7808 - val_loss: 0.6119 - val_acc: 0.7626\n",
      "Epoch 52/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.5992 - acc: 0.7828 - val_loss: 0.6108 - val_acc: 0.7626\n",
      "Epoch 53/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.5976 - acc: 0.7838 - val_loss: 0.6091 - val_acc: 0.7671\n",
      "Epoch 54/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.5961 - acc: 0.7818 - val_loss: 0.6075 - val_acc: 0.7717\n",
      "Epoch 55/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5944 - acc: 0.7847 - val_loss: 0.6056 - val_acc: 0.7717\n",
      "Epoch 56/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.5929 - acc: 0.7886 - val_loss: 0.6042 - val_acc: 0.7717\n",
      "Epoch 57/700\n",
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.5912 - acc: 0.7945 - val_loss: 0.6027 - val_acc: 0.7717\n",
      "Epoch 58/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.5896 - acc: 0.7955 - val_loss: 0.6012 - val_acc: 0.7717\n",
      "Epoch 59/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.5879 - acc: 0.7945 - val_loss: 0.6003 - val_acc: 0.7717\n",
      "Epoch 60/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.5863 - acc: 0.7926 - val_loss: 0.5986 - val_acc: 0.7717\n",
      "Epoch 61/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.5848 - acc: 0.7926 - val_loss: 0.5966 - val_acc: 0.7717\n",
      "Epoch 62/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.5833 - acc: 0.8004 - val_loss: 0.5952 - val_acc: 0.7717\n",
      "Epoch 63/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.5816 - acc: 0.8014 - val_loss: 0.5935 - val_acc: 0.7717\n",
      "Epoch 64/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.5800 - acc: 0.8023 - val_loss: 0.5919 - val_acc: 0.7717\n",
      "Epoch 65/700\n",
      "1022/1022 [==============================] - 0s 46us/step - loss: 0.5783 - acc: 0.8014 - val_loss: 0.5909 - val_acc: 0.7717\n",
      "Epoch 66/700\n",
      "1022/1022 [==============================] - 0s 43us/step - loss: 0.5767 - acc: 0.8014 - val_loss: 0.5900 - val_acc: 0.7763\n",
      "Epoch 67/700\n",
      "1022/1022 [==============================] - 0s 36us/step - loss: 0.5751 - acc: 0.7994 - val_loss: 0.5883 - val_acc: 0.7763\n",
      "Epoch 68/700\n",
      "1022/1022 [==============================] - 0s 37us/step - loss: 0.5734 - acc: 0.7994 - val_loss: 0.5863 - val_acc: 0.7763\n",
      "Epoch 69/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.5717 - acc: 0.8063 - val_loss: 0.5839 - val_acc: 0.7900\n",
      "Epoch 70/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.5702 - acc: 0.8082 - val_loss: 0.5827 - val_acc: 0.7808\n",
      "Epoch 71/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.5685 - acc: 0.8072 - val_loss: 0.5812 - val_acc: 0.7854\n",
      "Epoch 72/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.5669 - acc: 0.8092 - val_loss: 0.5797 - val_acc: 0.7900\n",
      "Epoch 73/700\n",
      "1022/1022 [==============================] - 0s 43us/step - loss: 0.5653 - acc: 0.8121 - val_loss: 0.5779 - val_acc: 0.7991\n",
      "Epoch 74/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.5637 - acc: 0.8112 - val_loss: 0.5768 - val_acc: 0.7900\n",
      "Epoch 75/700\n",
      "1022/1022 [==============================] - 0s 40us/step - loss: 0.5620 - acc: 0.8131 - val_loss: 0.5758 - val_acc: 0.7854\n",
      "Epoch 76/700\n",
      "1022/1022 [==============================] - 0s 41us/step - loss: 0.5603 - acc: 0.8102 - val_loss: 0.5739 - val_acc: 0.7900\n",
      "Epoch 77/700\n",
      "1022/1022 [==============================] - 0s 67us/step - loss: 0.5586 - acc: 0.8102 - val_loss: 0.5725 - val_acc: 0.7900\n",
      "Epoch 78/700\n",
      "1022/1022 [==============================] - 0s 42us/step - loss: 0.5569 - acc: 0.8121 - val_loss: 0.5709 - val_acc: 0.7991\n",
      "Epoch 79/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.5553 - acc: 0.8112 - val_loss: 0.5692 - val_acc: 0.7991\n",
      "Epoch 80/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5537 - acc: 0.8141 - val_loss: 0.5672 - val_acc: 0.7991\n",
      "Epoch 81/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.5521 - acc: 0.8121 - val_loss: 0.5657 - val_acc: 0.7991\n",
      "Epoch 82/700\n",
      "1022/1022 [==============================] - 0s 46us/step - loss: 0.5504 - acc: 0.8131 - val_loss: 0.5645 - val_acc: 0.8037\n",
      "Epoch 83/700\n",
      "1022/1022 [==============================] - 0s 35us/step - loss: 0.5488 - acc: 0.8160 - val_loss: 0.5636 - val_acc: 0.7991\n",
      "Epoch 84/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5471 - acc: 0.8121 - val_loss: 0.5624 - val_acc: 0.7991\n",
      "Epoch 85/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.5454 - acc: 0.8151 - val_loss: 0.5603 - val_acc: 0.7991\n",
      "Epoch 86/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.5438 - acc: 0.8141 - val_loss: 0.5597 - val_acc: 0.7991\n",
      "Epoch 87/700\n",
      "1022/1022 [==============================] - 0s 36us/step - loss: 0.5421 - acc: 0.8141 - val_loss: 0.5584 - val_acc: 0.7900\n",
      "Epoch 88/700\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.5338 - acc: 0.830 - 0s 39us/step - loss: 0.5405 - acc: 0.8121 - val_loss: 0.5565 - val_acc: 0.8037\n",
      "Epoch 89/700\n",
      "1022/1022 [==============================] - 0s 67us/step - loss: 0.5388 - acc: 0.8151 - val_loss: 0.5547 - val_acc: 0.8037\n",
      "Epoch 90/700\n",
      "1022/1022 [==============================] - 0s 61us/step - loss: 0.5371 - acc: 0.8151 - val_loss: 0.5525 - val_acc: 0.8082\n",
      "Epoch 91/700\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.5354 - acc: 0.8180 - val_loss: 0.5513 - val_acc: 0.8037\n",
      "Epoch 92/700\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.5337 - acc: 0.8170 - val_loss: 0.5492 - val_acc: 0.8037\n",
      "Epoch 93/700\n",
      "1022/1022 [==============================] - 0s 46us/step - loss: 0.5320 - acc: 0.8219 - val_loss: 0.5471 - val_acc: 0.8128\n",
      "Epoch 94/700\n",
      "1022/1022 [==============================] - 0s 39us/step - loss: 0.5303 - acc: 0.8229 - val_loss: 0.5453 - val_acc: 0.8128\n",
      "Epoch 95/700\n",
      "1022/1022 [==============================] - 0s 39us/step - loss: 0.5287 - acc: 0.8249 - val_loss: 0.5439 - val_acc: 0.8128\n",
      "Epoch 96/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.5270 - acc: 0.8258 - val_loss: 0.5419 - val_acc: 0.8128\n",
      "Epoch 97/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5252 - acc: 0.8258 - val_loss: 0.5404 - val_acc: 0.8128\n",
      "Epoch 98/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.5236 - acc: 0.8258 - val_loss: 0.5393 - val_acc: 0.8128\n",
      "Epoch 99/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5219 - acc: 0.8268 - val_loss: 0.5377 - val_acc: 0.8128\n",
      "Epoch 100/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.5202 - acc: 0.8249 - val_loss: 0.5353 - val_acc: 0.8128\n",
      "Epoch 101/700\n",
      "1022/1022 [==============================] - 0s 41us/step - loss: 0.5186 - acc: 0.8288 - val_loss: 0.5348 - val_acc: 0.8128\n",
      "Epoch 102/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.5169 - acc: 0.8258 - val_loss: 0.5331 - val_acc: 0.8128\n",
      "Epoch 103/700\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.5152 - acc: 0.8307 - val_loss: 0.5316 - val_acc: 0.8128\n",
      "Epoch 104/700\n",
      "1022/1022 [==============================] - 0s 38us/step - loss: 0.5135 - acc: 0.8268 - val_loss: 0.5291 - val_acc: 0.8128\n",
      "Epoch 105/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.5118 - acc: 0.8327 - val_loss: 0.5274 - val_acc: 0.8128\n",
      "Epoch 106/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.5099 - acc: 0.8327 - val_loss: 0.5259 - val_acc: 0.8128\n",
      "Epoch 107/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5083 - acc: 0.8297 - val_loss: 0.5239 - val_acc: 0.8082\n",
      "Epoch 108/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5066 - acc: 0.8317 - val_loss: 0.5221 - val_acc: 0.8128\n",
      "Epoch 109/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.5049 - acc: 0.8346 - val_loss: 0.5195 - val_acc: 0.8356\n",
      "Epoch 110/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.5035 - acc: 0.8297 - val_loss: 0.5181 - val_acc: 0.8356\n",
      "Epoch 111/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.5018 - acc: 0.8297 - val_loss: 0.5172 - val_acc: 0.8265\n",
      "Epoch 112/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.5001 - acc: 0.8317 - val_loss: 0.5153 - val_acc: 0.8356\n",
      "Epoch 113/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.4984 - acc: 0.8327 - val_loss: 0.5131 - val_acc: 0.8356\n",
      "Epoch 114/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.4969 - acc: 0.8307 - val_loss: 0.5124 - val_acc: 0.8311\n",
      "Epoch 115/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4950 - acc: 0.8337 - val_loss: 0.5111 - val_acc: 0.8265\n",
      "Epoch 116/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4933 - acc: 0.8356 - val_loss: 0.5091 - val_acc: 0.8311\n",
      "Epoch 117/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 41us/step - loss: 0.4916 - acc: 0.8337 - val_loss: 0.5069 - val_acc: 0.8402\n",
      "Epoch 118/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.4902 - acc: 0.8327 - val_loss: 0.5065 - val_acc: 0.8311\n",
      "Epoch 119/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.4884 - acc: 0.8356 - val_loss: 0.5062 - val_acc: 0.8174\n",
      "Epoch 120/700\n",
      "1022/1022 [==============================] - 0s 38us/step - loss: 0.4869 - acc: 0.8376 - val_loss: 0.5050 - val_acc: 0.8174\n",
      "Epoch 121/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.4853 - acc: 0.8376 - val_loss: 0.5039 - val_acc: 0.8219\n",
      "Epoch 122/700\n",
      "1022/1022 [==============================] - 0s 38us/step - loss: 0.4837 - acc: 0.8386 - val_loss: 0.5015 - val_acc: 0.8219\n",
      "Epoch 123/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4821 - acc: 0.8395 - val_loss: 0.5000 - val_acc: 0.8219\n",
      "Epoch 124/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4805 - acc: 0.8395 - val_loss: 0.4988 - val_acc: 0.8265\n",
      "Epoch 125/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.4790 - acc: 0.8395 - val_loss: 0.4974 - val_acc: 0.8265\n",
      "Epoch 126/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.4774 - acc: 0.8395 - val_loss: 0.4959 - val_acc: 0.8265\n",
      "Epoch 127/700\n",
      "1022/1022 [==============================] - 0s 43us/step - loss: 0.4757 - acc: 0.8405 - val_loss: 0.4938 - val_acc: 0.8311\n",
      "Epoch 128/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.4741 - acc: 0.8405 - val_loss: 0.4923 - val_acc: 0.8311\n",
      "Epoch 129/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.4725 - acc: 0.8386 - val_loss: 0.4908 - val_acc: 0.8311\n",
      "Epoch 130/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4709 - acc: 0.8415 - val_loss: 0.4882 - val_acc: 0.8356\n",
      "Epoch 131/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.4693 - acc: 0.8366 - val_loss: 0.4875 - val_acc: 0.8311\n",
      "Epoch 132/700\n",
      "1022/1022 [==============================] - 0s 44us/step - loss: 0.4677 - acc: 0.8425 - val_loss: 0.4856 - val_acc: 0.8356\n",
      "Epoch 133/700\n",
      "1022/1022 [==============================] - 0s 42us/step - loss: 0.4661 - acc: 0.8376 - val_loss: 0.4853 - val_acc: 0.8311\n",
      "Epoch 134/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.4646 - acc: 0.8405 - val_loss: 0.4821 - val_acc: 0.8356\n",
      "Epoch 135/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.4630 - acc: 0.8386 - val_loss: 0.4801 - val_acc: 0.8493\n",
      "Epoch 136/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.4614 - acc: 0.8366 - val_loss: 0.4781 - val_acc: 0.8539\n",
      "Epoch 137/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.4601 - acc: 0.8346 - val_loss: 0.4783 - val_acc: 0.8356\n",
      "Epoch 138/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.4584 - acc: 0.8415 - val_loss: 0.4772 - val_acc: 0.8356\n",
      "Epoch 139/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.4570 - acc: 0.8425 - val_loss: 0.4755 - val_acc: 0.8356\n",
      "Epoch 140/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.4554 - acc: 0.8405 - val_loss: 0.4740 - val_acc: 0.8356\n",
      "Epoch 141/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.4540 - acc: 0.8405 - val_loss: 0.4725 - val_acc: 0.8356\n",
      "Epoch 142/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.4526 - acc: 0.8405 - val_loss: 0.4710 - val_acc: 0.8356\n",
      "Epoch 143/700\n",
      "1022/1022 [==============================] - 0s 37us/step - loss: 0.4510 - acc: 0.8405 - val_loss: 0.4702 - val_acc: 0.8356\n",
      "Epoch 144/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.4496 - acc: 0.8425 - val_loss: 0.4689 - val_acc: 0.8356\n",
      "Epoch 145/700\n",
      "1022/1022 [==============================] - 0s 45us/step - loss: 0.4482 - acc: 0.8434 - val_loss: 0.4690 - val_acc: 0.8356\n",
      "Epoch 146/700\n",
      "1022/1022 [==============================] - 0s 39us/step - loss: 0.4471 - acc: 0.8425 - val_loss: 0.4659 - val_acc: 0.8356\n",
      "Epoch 147/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.4452 - acc: 0.8425 - val_loss: 0.4633 - val_acc: 0.8493\n",
      "Epoch 148/700\n",
      "1022/1022 [==============================] - 0s 35us/step - loss: 0.4438 - acc: 0.8425 - val_loss: 0.4621 - val_acc: 0.8493\n",
      "Epoch 149/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.4425 - acc: 0.8415 - val_loss: 0.4603 - val_acc: 0.8584\n",
      "Epoch 150/700\n",
      "1022/1022 [==============================] - 0s 38us/step - loss: 0.4411 - acc: 0.8434 - val_loss: 0.4587 - val_acc: 0.8584\n",
      "Epoch 151/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.4396 - acc: 0.8395 - val_loss: 0.4579 - val_acc: 0.8493\n",
      "Epoch 152/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4381 - acc: 0.8425 - val_loss: 0.4566 - val_acc: 0.8493\n",
      "Epoch 153/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4368 - acc: 0.8415 - val_loss: 0.4553 - val_acc: 0.8493\n",
      "Epoch 154/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.4354 - acc: 0.8434 - val_loss: 0.4540 - val_acc: 0.8493\n",
      "Epoch 155/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4342 - acc: 0.8425 - val_loss: 0.4523 - val_acc: 0.8539\n",
      "Epoch 156/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4328 - acc: 0.8425 - val_loss: 0.4504 - val_acc: 0.8584\n",
      "Epoch 157/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4316 - acc: 0.8434 - val_loss: 0.4495 - val_acc: 0.8539\n",
      "Epoch 158/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4302 - acc: 0.8425 - val_loss: 0.4488 - val_acc: 0.8493\n",
      "Epoch 159/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4289 - acc: 0.8444 - val_loss: 0.4480 - val_acc: 0.8493\n",
      "Epoch 160/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4277 - acc: 0.8454 - val_loss: 0.4467 - val_acc: 0.8539\n",
      "Epoch 161/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4265 - acc: 0.8444 - val_loss: 0.4456 - val_acc: 0.8539\n",
      "Epoch 162/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4252 - acc: 0.8474 - val_loss: 0.4440 - val_acc: 0.8493\n",
      "Epoch 163/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4239 - acc: 0.8464 - val_loss: 0.4425 - val_acc: 0.8539\n",
      "Epoch 164/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4226 - acc: 0.8454 - val_loss: 0.4415 - val_acc: 0.8493\n",
      "Epoch 165/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.4213 - acc: 0.8474 - val_loss: 0.4413 - val_acc: 0.8447\n",
      "Epoch 166/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.4201 - acc: 0.8454 - val_loss: 0.4395 - val_acc: 0.8493\n",
      "Epoch 167/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4188 - acc: 0.8474 - val_loss: 0.4386 - val_acc: 0.8539\n",
      "Epoch 168/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4175 - acc: 0.8493 - val_loss: 0.4373 - val_acc: 0.8493\n",
      "Epoch 169/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4163 - acc: 0.8493 - val_loss: 0.4359 - val_acc: 0.8493\n",
      "Epoch 170/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4151 - acc: 0.8474 - val_loss: 0.4343 - val_acc: 0.8493\n",
      "Epoch 171/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.4141 - acc: 0.8464 - val_loss: 0.4338 - val_acc: 0.8493\n",
      "Epoch 172/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4128 - acc: 0.8483 - val_loss: 0.4324 - val_acc: 0.8493\n",
      "Epoch 173/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.4117 - acc: 0.8464 - val_loss: 0.4310 - val_acc: 0.8493\n",
      "Epoch 174/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.4105 - acc: 0.8464 - val_loss: 0.4299 - val_acc: 0.8493\n",
      "Epoch 175/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.4093 - acc: 0.8493 - val_loss: 0.4292 - val_acc: 0.8493\n",
      "Epoch 176/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.4082 - acc: 0.8493 - val_loss: 0.4289 - val_acc: 0.8493\n",
      "Epoch 177/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.4073 - acc: 0.8493 - val_loss: 0.4288 - val_acc: 0.8447\n",
      "Epoch 178/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.4065 - acc: 0.8513 - val_loss: 0.4258 - val_acc: 0.8493\n",
      "Epoch 179/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.4050 - acc: 0.8503 - val_loss: 0.4249 - val_acc: 0.8493\n",
      "Epoch 180/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.4039 - acc: 0.8493 - val_loss: 0.4241 - val_acc: 0.8493\n",
      "Epoch 181/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4028 - acc: 0.8523 - val_loss: 0.4222 - val_acc: 0.8539\n",
      "Epoch 182/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.4016 - acc: 0.8464 - val_loss: 0.4212 - val_acc: 0.8539\n",
      "Epoch 183/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.4006 - acc: 0.8474 - val_loss: 0.4202 - val_acc: 0.8539\n",
      "Epoch 184/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3996 - acc: 0.8474 - val_loss: 0.4188 - val_acc: 0.8584\n",
      "Epoch 185/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3986 - acc: 0.8474 - val_loss: 0.4193 - val_acc: 0.8493\n",
      "Epoch 186/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3976 - acc: 0.8532 - val_loss: 0.4178 - val_acc: 0.8493\n",
      "Epoch 187/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3967 - acc: 0.8513 - val_loss: 0.4156 - val_acc: 0.8584\n",
      "Epoch 188/700\n",
      "1022/1022 [==============================] - 0s 36us/step - loss: 0.3954 - acc: 0.8474 - val_loss: 0.4151 - val_acc: 0.8539\n",
      "Epoch 189/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3944 - acc: 0.8493 - val_loss: 0.4143 - val_acc: 0.8539\n",
      "Epoch 190/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3937 - acc: 0.8474 - val_loss: 0.4127 - val_acc: 0.8584\n",
      "Epoch 191/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3927 - acc: 0.8523 - val_loss: 0.4123 - val_acc: 0.8539\n",
      "Epoch 192/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3917 - acc: 0.8513 - val_loss: 0.4110 - val_acc: 0.8584\n",
      "Epoch 193/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3906 - acc: 0.8513 - val_loss: 0.4097 - val_acc: 0.8630\n",
      "Epoch 194/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3898 - acc: 0.8523 - val_loss: 0.4085 - val_acc: 0.8676\n",
      "Epoch 195/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3892 - acc: 0.8523 - val_loss: 0.4083 - val_acc: 0.8584\n",
      "Epoch 196/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3881 - acc: 0.8483 - val_loss: 0.4081 - val_acc: 0.8539\n",
      "Epoch 197/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3872 - acc: 0.8542 - val_loss: 0.4076 - val_acc: 0.8539\n",
      "Epoch 198/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3864 - acc: 0.8532 - val_loss: 0.4067 - val_acc: 0.8539\n",
      "Epoch 199/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3855 - acc: 0.8532 - val_loss: 0.4071 - val_acc: 0.8539\n",
      "Epoch 200/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3848 - acc: 0.8542 - val_loss: 0.4062 - val_acc: 0.8539\n",
      "Epoch 201/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.3840 - acc: 0.8552 - val_loss: 0.4046 - val_acc: 0.8539\n",
      "Epoch 202/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3829 - acc: 0.8542 - val_loss: 0.4038 - val_acc: 0.8539\n",
      "Epoch 203/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3821 - acc: 0.8552 - val_loss: 0.4015 - val_acc: 0.8584\n",
      "Epoch 204/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3811 - acc: 0.8552 - val_loss: 0.4002 - val_acc: 0.8630\n",
      "Epoch 205/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3802 - acc: 0.8562 - val_loss: 0.4002 - val_acc: 0.8539\n",
      "Epoch 206/700\n",
      "1022/1022 [==============================] - 0s 36us/step - loss: 0.3794 - acc: 0.8542 - val_loss: 0.4001 - val_acc: 0.8539\n",
      "Epoch 207/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3785 - acc: 0.8552 - val_loss: 0.3985 - val_acc: 0.8539\n",
      "Epoch 208/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3777 - acc: 0.8552 - val_loss: 0.3975 - val_acc: 0.8584\n",
      "Epoch 209/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3769 - acc: 0.8552 - val_loss: 0.3957 - val_acc: 0.8630\n",
      "Epoch 210/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3764 - acc: 0.8552 - val_loss: 0.3961 - val_acc: 0.8539\n",
      "Epoch 211/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3753 - acc: 0.8552 - val_loss: 0.3953 - val_acc: 0.8539\n",
      "Epoch 212/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3746 - acc: 0.8562 - val_loss: 0.3958 - val_acc: 0.8584\n",
      "Epoch 213/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3740 - acc: 0.8581 - val_loss: 0.3961 - val_acc: 0.8584\n",
      "Epoch 214/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3735 - acc: 0.8562 - val_loss: 0.3952 - val_acc: 0.8584\n",
      "Epoch 215/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3727 - acc: 0.8571 - val_loss: 0.3936 - val_acc: 0.8584\n",
      "Epoch 216/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3721 - acc: 0.8591 - val_loss: 0.3922 - val_acc: 0.8539\n",
      "Epoch 217/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3711 - acc: 0.8581 - val_loss: 0.3921 - val_acc: 0.8584\n",
      "Epoch 218/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3703 - acc: 0.8591 - val_loss: 0.3913 - val_acc: 0.8539\n",
      "Epoch 219/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3696 - acc: 0.8611 - val_loss: 0.3894 - val_acc: 0.8584\n",
      "Epoch 220/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3688 - acc: 0.8571 - val_loss: 0.3902 - val_acc: 0.8584\n",
      "Epoch 221/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3683 - acc: 0.8601 - val_loss: 0.3918 - val_acc: 0.8584\n",
      "Epoch 222/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3684 - acc: 0.8581 - val_loss: 0.3893 - val_acc: 0.8584\n",
      "Epoch 223/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3670 - acc: 0.8591 - val_loss: 0.3871 - val_acc: 0.8539\n",
      "Epoch 224/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.3661 - acc: 0.8611 - val_loss: 0.3867 - val_acc: 0.8539\n",
      "Epoch 225/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3657 - acc: 0.8601 - val_loss: 0.3856 - val_acc: 0.8539\n",
      "Epoch 226/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3647 - acc: 0.8601 - val_loss: 0.3849 - val_acc: 0.8539\n",
      "Epoch 227/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3641 - acc: 0.8601 - val_loss: 0.3838 - val_acc: 0.8539\n",
      "Epoch 228/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3636 - acc: 0.8601 - val_loss: 0.3825 - val_acc: 0.8584\n",
      "Epoch 229/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3629 - acc: 0.8571 - val_loss: 0.3827 - val_acc: 0.8539\n",
      "Epoch 230/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3620 - acc: 0.8601 - val_loss: 0.3819 - val_acc: 0.8539\n",
      "Epoch 231/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3615 - acc: 0.8591 - val_loss: 0.3823 - val_acc: 0.8584\n",
      "Epoch 232/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3610 - acc: 0.8630 - val_loss: 0.3811 - val_acc: 0.8493\n",
      "Epoch 233/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3604 - acc: 0.8601 - val_loss: 0.3804 - val_acc: 0.8493\n",
      "Epoch 234/700\n",
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.3596 - acc: 0.8601 - val_loss: 0.3794 - val_acc: 0.8539\n",
      "Epoch 235/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3591 - acc: 0.8611 - val_loss: 0.3803 - val_acc: 0.8584\n",
      "Epoch 236/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3585 - acc: 0.8630 - val_loss: 0.3776 - val_acc: 0.8584\n",
      "Epoch 237/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3581 - acc: 0.8601 - val_loss: 0.3768 - val_acc: 0.8584\n",
      "Epoch 238/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3575 - acc: 0.8601 - val_loss: 0.3763 - val_acc: 0.8584\n",
      "Epoch 239/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3571 - acc: 0.8562 - val_loss: 0.3759 - val_acc: 0.8539\n",
      "Epoch 240/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.3561 - acc: 0.8591 - val_loss: 0.3772 - val_acc: 0.8584\n",
      "Epoch 241/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3557 - acc: 0.8640 - val_loss: 0.3763 - val_acc: 0.8584\n",
      "Epoch 242/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3551 - acc: 0.8640 - val_loss: 0.3755 - val_acc: 0.8539\n",
      "Epoch 243/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3543 - acc: 0.8659 - val_loss: 0.3743 - val_acc: 0.8539\n",
      "Epoch 244/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3538 - acc: 0.8640 - val_loss: 0.3744 - val_acc: 0.8539\n",
      "Epoch 245/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3533 - acc: 0.8659 - val_loss: 0.3746 - val_acc: 0.8584\n",
      "Epoch 246/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3529 - acc: 0.8650 - val_loss: 0.3733 - val_acc: 0.8539\n",
      "Epoch 247/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3522 - acc: 0.8659 - val_loss: 0.3726 - val_acc: 0.8539\n",
      "Epoch 248/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3515 - acc: 0.8650 - val_loss: 0.3732 - val_acc: 0.8584\n",
      "Epoch 249/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3514 - acc: 0.8689 - val_loss: 0.3725 - val_acc: 0.8584\n",
      "Epoch 250/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3508 - acc: 0.8679 - val_loss: 0.3703 - val_acc: 0.8584\n",
      "Epoch 251/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3500 - acc: 0.8650 - val_loss: 0.3691 - val_acc: 0.8539\n",
      "Epoch 252/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3495 - acc: 0.8650 - val_loss: 0.3687 - val_acc: 0.8539\n",
      "Epoch 253/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3489 - acc: 0.8640 - val_loss: 0.3686 - val_acc: 0.8584\n",
      "Epoch 254/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3482 - acc: 0.8650 - val_loss: 0.3681 - val_acc: 0.8584\n",
      "Epoch 255/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3476 - acc: 0.8640 - val_loss: 0.3684 - val_acc: 0.8539\n",
      "Epoch 256/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3473 - acc: 0.8669 - val_loss: 0.3677 - val_acc: 0.8539\n",
      "Epoch 257/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3468 - acc: 0.8659 - val_loss: 0.3667 - val_acc: 0.8539\n",
      "Epoch 258/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3463 - acc: 0.8650 - val_loss: 0.3658 - val_acc: 0.8584\n",
      "Epoch 259/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3460 - acc: 0.8650 - val_loss: 0.3650 - val_acc: 0.8584\n",
      "Epoch 260/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3452 - acc: 0.8630 - val_loss: 0.3650 - val_acc: 0.8539\n",
      "Epoch 261/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3447 - acc: 0.8659 - val_loss: 0.3639 - val_acc: 0.8584\n",
      "Epoch 262/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3443 - acc: 0.8679 - val_loss: 0.3639 - val_acc: 0.8584\n",
      "Epoch 263/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3437 - acc: 0.8669 - val_loss: 0.3633 - val_acc: 0.8584\n",
      "Epoch 264/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3433 - acc: 0.8650 - val_loss: 0.3633 - val_acc: 0.8539\n",
      "Epoch 265/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3428 - acc: 0.8659 - val_loss: 0.3630 - val_acc: 0.8539\n",
      "Epoch 266/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3425 - acc: 0.8659 - val_loss: 0.3620 - val_acc: 0.8584\n",
      "Epoch 267/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3418 - acc: 0.8659 - val_loss: 0.3612 - val_acc: 0.8584\n",
      "Epoch 268/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3415 - acc: 0.8659 - val_loss: 0.3622 - val_acc: 0.8539\n",
      "Epoch 269/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3412 - acc: 0.8679 - val_loss: 0.3615 - val_acc: 0.8539\n",
      "Epoch 270/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3407 - acc: 0.8650 - val_loss: 0.3607 - val_acc: 0.8539\n",
      "Epoch 271/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3402 - acc: 0.8640 - val_loss: 0.3588 - val_acc: 0.8493\n",
      "Epoch 272/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3400 - acc: 0.8659 - val_loss: 0.3590 - val_acc: 0.8584\n",
      "Epoch 273/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3394 - acc: 0.8679 - val_loss: 0.3578 - val_acc: 0.8584\n",
      "Epoch 274/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3394 - acc: 0.8659 - val_loss: 0.3574 - val_acc: 0.8539\n",
      "Epoch 275/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3387 - acc: 0.8669 - val_loss: 0.3571 - val_acc: 0.8493\n",
      "Epoch 276/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3383 - acc: 0.8650 - val_loss: 0.3567 - val_acc: 0.8493\n",
      "Epoch 277/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3377 - acc: 0.8669 - val_loss: 0.3573 - val_acc: 0.8584\n",
      "Epoch 278/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3372 - acc: 0.8689 - val_loss: 0.3561 - val_acc: 0.8584\n",
      "Epoch 279/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.3370 - acc: 0.8659 - val_loss: 0.3559 - val_acc: 0.8584\n",
      "Epoch 280/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3366 - acc: 0.8679 - val_loss: 0.3570 - val_acc: 0.8539\n",
      "Epoch 281/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3362 - acc: 0.8699 - val_loss: 0.3571 - val_acc: 0.8539\n",
      "Epoch 282/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3359 - acc: 0.8699 - val_loss: 0.3571 - val_acc: 0.8539\n",
      "Epoch 283/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3356 - acc: 0.8708 - val_loss: 0.3544 - val_acc: 0.8584\n",
      "Epoch 284/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3350 - acc: 0.8669 - val_loss: 0.3548 - val_acc: 0.8539\n",
      "Epoch 285/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.3348 - acc: 0.8699 - val_loss: 0.3542 - val_acc: 0.8584\n",
      "Epoch 286/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3342 - acc: 0.8669 - val_loss: 0.3541 - val_acc: 0.8539\n",
      "Epoch 287/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3338 - acc: 0.8718 - val_loss: 0.3535 - val_acc: 0.8584\n",
      "Epoch 288/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3334 - acc: 0.8689 - val_loss: 0.3528 - val_acc: 0.8584\n",
      "Epoch 289/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.3332 - acc: 0.8699 - val_loss: 0.3523 - val_acc: 0.8584\n",
      "Epoch 290/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3327 - acc: 0.8689 - val_loss: 0.3510 - val_acc: 0.8539\n",
      "Epoch 291/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3326 - acc: 0.8679 - val_loss: 0.3505 - val_acc: 0.8539\n",
      "Epoch 292/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3324 - acc: 0.8679 - val_loss: 0.3512 - val_acc: 0.8584\n",
      "Epoch 293/700\n",
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.3315 - acc: 0.8708 - val_loss: 0.3499 - val_acc: 0.8584\n",
      "Epoch 294/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3316 - acc: 0.8699 - val_loss: 0.3521 - val_acc: 0.8539\n",
      "Epoch 295/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3312 - acc: 0.8708 - val_loss: 0.3515 - val_acc: 0.8539\n",
      "Epoch 296/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.3307 - acc: 0.8728 - val_loss: 0.3500 - val_acc: 0.8539\n",
      "Epoch 297/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3302 - acc: 0.8708 - val_loss: 0.3492 - val_acc: 0.8584\n",
      "Epoch 298/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3301 - acc: 0.8699 - val_loss: 0.3491 - val_acc: 0.8584\n",
      "Epoch 299/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3294 - acc: 0.8718 - val_loss: 0.3482 - val_acc: 0.8630\n",
      "Epoch 300/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3292 - acc: 0.8699 - val_loss: 0.3476 - val_acc: 0.8630\n",
      "Epoch 301/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3289 - acc: 0.8708 - val_loss: 0.3487 - val_acc: 0.8539\n",
      "Epoch 302/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3286 - acc: 0.8738 - val_loss: 0.3469 - val_acc: 0.8630\n",
      "Epoch 303/700\n",
      "1022/1022 [==============================] - 0s 44us/step - loss: 0.3284 - acc: 0.8708 - val_loss: 0.3487 - val_acc: 0.8539\n",
      "Epoch 304/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3282 - acc: 0.8718 - val_loss: 0.3473 - val_acc: 0.8539\n",
      "Epoch 305/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3276 - acc: 0.8728 - val_loss: 0.3457 - val_acc: 0.8630\n",
      "Epoch 306/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3271 - acc: 0.8748 - val_loss: 0.3450 - val_acc: 0.8584\n",
      "Epoch 307/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3277 - acc: 0.8689 - val_loss: 0.3457 - val_acc: 0.8630\n",
      "Epoch 308/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3265 - acc: 0.8738 - val_loss: 0.3469 - val_acc: 0.8539\n",
      "Epoch 309/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3265 - acc: 0.8728 - val_loss: 0.3453 - val_acc: 0.8539\n",
      "Epoch 310/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3260 - acc: 0.8728 - val_loss: 0.3437 - val_acc: 0.8584\n",
      "Epoch 311/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3257 - acc: 0.8699 - val_loss: 0.3442 - val_acc: 0.8630\n",
      "Epoch 312/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3252 - acc: 0.8728 - val_loss: 0.3433 - val_acc: 0.8584\n",
      "Epoch 313/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3251 - acc: 0.8748 - val_loss: 0.3429 - val_acc: 0.8584\n",
      "Epoch 314/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3247 - acc: 0.8738 - val_loss: 0.3425 - val_acc: 0.8584\n",
      "Epoch 315/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3246 - acc: 0.8718 - val_loss: 0.3435 - val_acc: 0.8539\n",
      "Epoch 316/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3241 - acc: 0.8738 - val_loss: 0.3434 - val_acc: 0.8539\n",
      "Epoch 317/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3239 - acc: 0.8718 - val_loss: 0.3432 - val_acc: 0.8539\n",
      "Epoch 318/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3238 - acc: 0.8738 - val_loss: 0.3423 - val_acc: 0.8584\n",
      "Epoch 319/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3233 - acc: 0.8757 - val_loss: 0.3433 - val_acc: 0.8539\n",
      "Epoch 320/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3233 - acc: 0.8738 - val_loss: 0.3434 - val_acc: 0.8539\n",
      "Epoch 321/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3231 - acc: 0.8728 - val_loss: 0.3417 - val_acc: 0.8539\n",
      "Epoch 322/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3226 - acc: 0.8738 - val_loss: 0.3414 - val_acc: 0.8584\n",
      "Epoch 323/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3223 - acc: 0.8767 - val_loss: 0.3401 - val_acc: 0.8630\n",
      "Epoch 324/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3219 - acc: 0.8757 - val_loss: 0.3405 - val_acc: 0.8630\n",
      "Epoch 325/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3221 - acc: 0.8748 - val_loss: 0.3406 - val_acc: 0.8584\n",
      "Epoch 326/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3214 - acc: 0.8757 - val_loss: 0.3390 - val_acc: 0.8630\n",
      "Epoch 327/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3212 - acc: 0.8757 - val_loss: 0.3391 - val_acc: 0.8630\n",
      "Epoch 328/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3207 - acc: 0.8757 - val_loss: 0.3394 - val_acc: 0.8584\n",
      "Epoch 329/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3206 - acc: 0.8767 - val_loss: 0.3385 - val_acc: 0.8630\n",
      "Epoch 330/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3201 - acc: 0.8757 - val_loss: 0.3374 - val_acc: 0.8584\n",
      "Epoch 331/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3202 - acc: 0.8767 - val_loss: 0.3371 - val_acc: 0.8584\n",
      "Epoch 332/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3199 - acc: 0.8738 - val_loss: 0.3369 - val_acc: 0.8584\n",
      "Epoch 333/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3194 - acc: 0.8748 - val_loss: 0.3367 - val_acc: 0.8584\n",
      "Epoch 334/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3192 - acc: 0.8757 - val_loss: 0.3361 - val_acc: 0.8676\n",
      "Epoch 335/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3196 - acc: 0.8689 - val_loss: 0.3359 - val_acc: 0.8630\n",
      "Epoch 336/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3188 - acc: 0.8728 - val_loss: 0.3362 - val_acc: 0.8630\n",
      "Epoch 337/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3182 - acc: 0.8767 - val_loss: 0.3357 - val_acc: 0.8630\n",
      "Epoch 338/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3179 - acc: 0.8767 - val_loss: 0.3358 - val_acc: 0.8630\n",
      "Epoch 339/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3176 - acc: 0.8767 - val_loss: 0.3362 - val_acc: 0.8584\n",
      "Epoch 340/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3176 - acc: 0.8748 - val_loss: 0.3366 - val_acc: 0.8584\n",
      "Epoch 341/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3174 - acc: 0.8767 - val_loss: 0.3370 - val_acc: 0.8539\n",
      "Epoch 342/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3172 - acc: 0.8728 - val_loss: 0.3359 - val_acc: 0.8584\n",
      "Epoch 343/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3168 - acc: 0.8757 - val_loss: 0.3340 - val_acc: 0.8630\n",
      "Epoch 344/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3163 - acc: 0.8757 - val_loss: 0.3346 - val_acc: 0.8630\n",
      "Epoch 345/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3161 - acc: 0.8777 - val_loss: 0.3336 - val_acc: 0.8630\n",
      "Epoch 346/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3159 - acc: 0.8767 - val_loss: 0.3339 - val_acc: 0.8630\n",
      "Epoch 347/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.3157 - acc: 0.8767 - val_loss: 0.3332 - val_acc: 0.8630\n",
      "Epoch 348/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3154 - acc: 0.8767 - val_loss: 0.3341 - val_acc: 0.8584\n",
      "Epoch 349/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3152 - acc: 0.8787 - val_loss: 0.3323 - val_acc: 0.8630\n",
      "Epoch 350/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3148 - acc: 0.8787 - val_loss: 0.3334 - val_acc: 0.8584\n",
      "Epoch 351/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3148 - acc: 0.8767 - val_loss: 0.3334 - val_acc: 0.8584\n",
      "Epoch 352/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3146 - acc: 0.8777 - val_loss: 0.3324 - val_acc: 0.8630\n",
      "Epoch 353/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.3142 - acc: 0.8796 - val_loss: 0.3317 - val_acc: 0.8630\n",
      "Epoch 354/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3138 - acc: 0.8777 - val_loss: 0.3312 - val_acc: 0.8630\n",
      "Epoch 355/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3138 - acc: 0.8796 - val_loss: 0.3309 - val_acc: 0.8630\n",
      "Epoch 356/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3134 - acc: 0.8777 - val_loss: 0.3300 - val_acc: 0.8630\n",
      "Epoch 357/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3135 - acc: 0.8748 - val_loss: 0.3297 - val_acc: 0.8630\n",
      "Epoch 358/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.3131 - acc: 0.8748 - val_loss: 0.3304 - val_acc: 0.8630\n",
      "Epoch 359/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3126 - acc: 0.8787 - val_loss: 0.3292 - val_acc: 0.8630\n",
      "Epoch 360/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3129 - acc: 0.8738 - val_loss: 0.3294 - val_acc: 0.8630\n",
      "Epoch 361/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3123 - acc: 0.8787 - val_loss: 0.3294 - val_acc: 0.8630\n",
      "Epoch 362/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3120 - acc: 0.8767 - val_loss: 0.3303 - val_acc: 0.8630\n",
      "Epoch 363/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3119 - acc: 0.8787 - val_loss: 0.3314 - val_acc: 0.8539\n",
      "Epoch 364/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.3121 - acc: 0.8777 - val_loss: 0.3304 - val_acc: 0.8539\n",
      "Epoch 365/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3116 - acc: 0.8767 - val_loss: 0.3285 - val_acc: 0.8630\n",
      "Epoch 366/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3111 - acc: 0.8796 - val_loss: 0.3284 - val_acc: 0.8630\n",
      "Epoch 367/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3111 - acc: 0.8787 - val_loss: 0.3273 - val_acc: 0.8630\n",
      "Epoch 368/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.3108 - acc: 0.8767 - val_loss: 0.3272 - val_acc: 0.8676\n",
      "Epoch 369/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3105 - acc: 0.8767 - val_loss: 0.3272 - val_acc: 0.8630\n",
      "Epoch 370/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3105 - acc: 0.8816 - val_loss: 0.3265 - val_acc: 0.8630\n",
      "Epoch 371/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3101 - acc: 0.8757 - val_loss: 0.3261 - val_acc: 0.8630\n",
      "Epoch 372/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3102 - acc: 0.8767 - val_loss: 0.3260 - val_acc: 0.8630\n",
      "Epoch 373/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3097 - acc: 0.8777 - val_loss: 0.3256 - val_acc: 0.8676\n",
      "Epoch 374/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3096 - acc: 0.8757 - val_loss: 0.3255 - val_acc: 0.8630\n",
      "Epoch 375/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3095 - acc: 0.8767 - val_loss: 0.3256 - val_acc: 0.8676\n",
      "Epoch 376/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3091 - acc: 0.8787 - val_loss: 0.3264 - val_acc: 0.8630\n",
      "Epoch 377/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3088 - acc: 0.8787 - val_loss: 0.3256 - val_acc: 0.8630\n",
      "Epoch 378/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3085 - acc: 0.8796 - val_loss: 0.3257 - val_acc: 0.8630\n",
      "Epoch 379/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3083 - acc: 0.8796 - val_loss: 0.3245 - val_acc: 0.8676\n",
      "Epoch 380/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3082 - acc: 0.8787 - val_loss: 0.3242 - val_acc: 0.8630\n",
      "Epoch 381/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3083 - acc: 0.8777 - val_loss: 0.3238 - val_acc: 0.8630\n",
      "Epoch 382/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3084 - acc: 0.8777 - val_loss: 0.3243 - val_acc: 0.8721\n",
      "Epoch 383/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3095 - acc: 0.8718 - val_loss: 0.3233 - val_acc: 0.8630\n",
      "Epoch 384/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3078 - acc: 0.8787 - val_loss: 0.3233 - val_acc: 0.8676\n",
      "Epoch 385/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3073 - acc: 0.8787 - val_loss: 0.3229 - val_acc: 0.8630\n",
      "Epoch 386/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3073 - acc: 0.8787 - val_loss: 0.3231 - val_acc: 0.8676\n",
      "Epoch 387/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3068 - acc: 0.8796 - val_loss: 0.3225 - val_acc: 0.8630\n",
      "Epoch 388/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3070 - acc: 0.8777 - val_loss: 0.3223 - val_acc: 0.8630\n",
      "Epoch 389/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.3066 - acc: 0.8787 - val_loss: 0.3226 - val_acc: 0.8676\n",
      "Epoch 390/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3062 - acc: 0.8796 - val_loss: 0.3226 - val_acc: 0.8630\n",
      "Epoch 391/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3061 - acc: 0.8787 - val_loss: 0.3225 - val_acc: 0.8630\n",
      "Epoch 392/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8777 - val_loss: 0.3216 - val_acc: 0.8676\n",
      "Epoch 393/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3056 - acc: 0.8787 - val_loss: 0.3213 - val_acc: 0.8676\n",
      "Epoch 394/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3055 - acc: 0.8796 - val_loss: 0.3211 - val_acc: 0.8676\n",
      "Epoch 395/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3056 - acc: 0.8787 - val_loss: 0.3211 - val_acc: 0.8676\n",
      "Epoch 396/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3053 - acc: 0.8787 - val_loss: 0.3219 - val_acc: 0.8584\n",
      "Epoch 397/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3052 - acc: 0.8796 - val_loss: 0.3213 - val_acc: 0.8584\n",
      "Epoch 398/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3046 - acc: 0.8796 - val_loss: 0.3206 - val_acc: 0.8676\n",
      "Epoch 399/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3045 - acc: 0.8787 - val_loss: 0.3205 - val_acc: 0.8676\n",
      "Epoch 400/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3043 - acc: 0.8787 - val_loss: 0.3202 - val_acc: 0.8676\n",
      "Epoch 401/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3041 - acc: 0.8806 - val_loss: 0.3198 - val_acc: 0.8676\n",
      "Epoch 402/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3046 - acc: 0.8787 - val_loss: 0.3193 - val_acc: 0.8630\n",
      "Epoch 403/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3038 - acc: 0.8796 - val_loss: 0.3190 - val_acc: 0.8630\n",
      "Epoch 404/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.3039 - acc: 0.8787 - val_loss: 0.3189 - val_acc: 0.8676\n",
      "Epoch 405/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3034 - acc: 0.8796 - val_loss: 0.3187 - val_acc: 0.8630\n",
      "Epoch 406/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3034 - acc: 0.8796 - val_loss: 0.3188 - val_acc: 0.8676\n",
      "Epoch 407/700\n",
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.3032 - acc: 0.8796 - val_loss: 0.3190 - val_acc: 0.8630\n",
      "Epoch 408/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3029 - acc: 0.8796 - val_loss: 0.3188 - val_acc: 0.8630\n",
      "Epoch 409/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3028 - acc: 0.8806 - val_loss: 0.3179 - val_acc: 0.8630\n",
      "Epoch 410/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.3026 - acc: 0.8806 - val_loss: 0.3192 - val_acc: 0.8584\n",
      "Epoch 411/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3027 - acc: 0.8787 - val_loss: 0.3175 - val_acc: 0.8584\n",
      "Epoch 412/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.3025 - acc: 0.8787 - val_loss: 0.3172 - val_acc: 0.8630\n",
      "Epoch 413/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.3024 - acc: 0.8787 - val_loss: 0.3174 - val_acc: 0.8630\n",
      "Epoch 414/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3019 - acc: 0.8787 - val_loss: 0.3167 - val_acc: 0.8630\n",
      "Epoch 415/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3021 - acc: 0.8796 - val_loss: 0.3165 - val_acc: 0.8630\n",
      "Epoch 416/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.3021 - acc: 0.8787 - val_loss: 0.3164 - val_acc: 0.8630\n",
      "Epoch 417/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.3016 - acc: 0.8806 - val_loss: 0.3164 - val_acc: 0.8630\n",
      "Epoch 418/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3012 - acc: 0.8796 - val_loss: 0.3166 - val_acc: 0.8630\n",
      "Epoch 419/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3011 - acc: 0.8806 - val_loss: 0.3161 - val_acc: 0.8630\n",
      "Epoch 420/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.3008 - acc: 0.8787 - val_loss: 0.3160 - val_acc: 0.8630\n",
      "Epoch 421/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.3007 - acc: 0.8796 - val_loss: 0.3160 - val_acc: 0.8630\n",
      "Epoch 422/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3006 - acc: 0.8796 - val_loss: 0.3167 - val_acc: 0.8584\n",
      "Epoch 423/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3006 - acc: 0.8816 - val_loss: 0.3165 - val_acc: 0.8584\n",
      "Epoch 424/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.3004 - acc: 0.8806 - val_loss: 0.3151 - val_acc: 0.8584\n",
      "Epoch 425/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3000 - acc: 0.8796 - val_loss: 0.3146 - val_acc: 0.8630\n",
      "Epoch 426/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.3002 - acc: 0.8796 - val_loss: 0.3144 - val_acc: 0.8630\n",
      "Epoch 427/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2998 - acc: 0.8806 - val_loss: 0.3160 - val_acc: 0.8584\n",
      "Epoch 428/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2998 - acc: 0.8816 - val_loss: 0.3145 - val_acc: 0.8630\n",
      "Epoch 429/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2993 - acc: 0.8796 - val_loss: 0.3149 - val_acc: 0.8630\n",
      "Epoch 430/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2993 - acc: 0.8806 - val_loss: 0.3171 - val_acc: 0.8539\n",
      "Epoch 431/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2997 - acc: 0.8826 - val_loss: 0.3183 - val_acc: 0.8539\n",
      "Epoch 432/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.3001 - acc: 0.8816 - val_loss: 0.3153 - val_acc: 0.8584\n",
      "Epoch 433/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2989 - acc: 0.8816 - val_loss: 0.3140 - val_acc: 0.8630\n",
      "Epoch 434/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2985 - acc: 0.8796 - val_loss: 0.3136 - val_acc: 0.8630\n",
      "Epoch 435/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2983 - acc: 0.8777 - val_loss: 0.3135 - val_acc: 0.8630\n",
      "Epoch 436/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2983 - acc: 0.8787 - val_loss: 0.3130 - val_acc: 0.8584\n",
      "Epoch 437/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2982 - acc: 0.8787 - val_loss: 0.3134 - val_acc: 0.8630\n",
      "Epoch 438/700\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.2957 - acc: 0.880 - 0s 26us/step - loss: 0.2979 - acc: 0.8826 - val_loss: 0.3127 - val_acc: 0.8584\n",
      "Epoch 439/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2976 - acc: 0.8777 - val_loss: 0.3123 - val_acc: 0.8584\n",
      "Epoch 440/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2978 - acc: 0.8767 - val_loss: 0.3123 - val_acc: 0.8584\n",
      "Epoch 441/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2974 - acc: 0.8777 - val_loss: 0.3129 - val_acc: 0.8630\n",
      "Epoch 442/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2973 - acc: 0.8806 - val_loss: 0.3132 - val_acc: 0.8630\n",
      "Epoch 443/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2972 - acc: 0.8836 - val_loss: 0.3127 - val_acc: 0.8630\n",
      "Epoch 444/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2971 - acc: 0.8836 - val_loss: 0.3121 - val_acc: 0.8630\n",
      "Epoch 445/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2967 - acc: 0.8806 - val_loss: 0.3114 - val_acc: 0.8584\n",
      "Epoch 446/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2968 - acc: 0.8777 - val_loss: 0.3110 - val_acc: 0.8630\n",
      "Epoch 447/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2965 - acc: 0.8796 - val_loss: 0.3118 - val_acc: 0.8630\n",
      "Epoch 448/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2963 - acc: 0.8796 - val_loss: 0.3127 - val_acc: 0.8584\n",
      "Epoch 449/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2966 - acc: 0.8826 - val_loss: 0.3113 - val_acc: 0.8630\n",
      "Epoch 450/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2961 - acc: 0.8796 - val_loss: 0.3104 - val_acc: 0.8630\n",
      "Epoch 451/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2958 - acc: 0.8777 - val_loss: 0.3114 - val_acc: 0.8630\n",
      "Epoch 452/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2962 - acc: 0.8806 - val_loss: 0.3098 - val_acc: 0.8630\n",
      "Epoch 453/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2958 - acc: 0.8806 - val_loss: 0.3098 - val_acc: 0.8630\n",
      "Epoch 454/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.2959 - acc: 0.8796 - val_loss: 0.3097 - val_acc: 0.8767\n",
      "Epoch 455/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2965 - acc: 0.8806 - val_loss: 0.3093 - val_acc: 0.8767\n",
      "Epoch 456/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2957 - acc: 0.8796 - val_loss: 0.3093 - val_acc: 0.8630\n",
      "Epoch 457/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2953 - acc: 0.8796 - val_loss: 0.3090 - val_acc: 0.8767\n",
      "Epoch 458/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2952 - acc: 0.8796 - val_loss: 0.3092 - val_acc: 0.8630\n",
      "Epoch 459/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.2948 - acc: 0.8787 - val_loss: 0.3087 - val_acc: 0.8721\n",
      "Epoch 460/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2954 - acc: 0.8787 - val_loss: 0.3085 - val_acc: 0.8767\n",
      "Epoch 461/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2950 - acc: 0.8787 - val_loss: 0.3084 - val_acc: 0.8676\n",
      "Epoch 462/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2945 - acc: 0.8796 - val_loss: 0.3082 - val_acc: 0.8676\n",
      "Epoch 463/700\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.2734 - acc: 0.890 - 0s 20us/step - loss: 0.2943 - acc: 0.8796 - val_loss: 0.3085 - val_acc: 0.8630\n",
      "Epoch 464/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2943 - acc: 0.8777 - val_loss: 0.3093 - val_acc: 0.8630\n",
      "Epoch 465/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2940 - acc: 0.8796 - val_loss: 0.3087 - val_acc: 0.8630\n",
      "Epoch 466/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2939 - acc: 0.8796 - val_loss: 0.3095 - val_acc: 0.8630\n",
      "Epoch 467/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2938 - acc: 0.8826 - val_loss: 0.3075 - val_acc: 0.8630\n",
      "Epoch 468/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2935 - acc: 0.8796 - val_loss: 0.3078 - val_acc: 0.8584\n",
      "Epoch 469/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2932 - acc: 0.8787 - val_loss: 0.3077 - val_acc: 0.8630\n",
      "Epoch 470/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2931 - acc: 0.8796 - val_loss: 0.3080 - val_acc: 0.8630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2931 - acc: 0.8796 - val_loss: 0.3074 - val_acc: 0.8630\n",
      "Epoch 472/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2931 - acc: 0.8787 - val_loss: 0.3068 - val_acc: 0.8584\n",
      "Epoch 473/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2928 - acc: 0.8796 - val_loss: 0.3070 - val_acc: 0.8584\n",
      "Epoch 474/700\n",
      "1022/1022 [==============================] - 0s 18us/step - loss: 0.2928 - acc: 0.8787 - val_loss: 0.3060 - val_acc: 0.8813\n",
      "Epoch 475/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2933 - acc: 0.8796 - val_loss: 0.3059 - val_acc: 0.8767\n",
      "Epoch 476/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2929 - acc: 0.8787 - val_loss: 0.3060 - val_acc: 0.8630\n",
      "Epoch 477/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2923 - acc: 0.8796 - val_loss: 0.3055 - val_acc: 0.8767\n",
      "Epoch 478/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2927 - acc: 0.8796 - val_loss: 0.3055 - val_acc: 0.8721\n",
      "Epoch 479/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2923 - acc: 0.8796 - val_loss: 0.3061 - val_acc: 0.8584\n",
      "Epoch 480/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2919 - acc: 0.8806 - val_loss: 0.3061 - val_acc: 0.8630\n",
      "Epoch 481/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2918 - acc: 0.8826 - val_loss: 0.3067 - val_acc: 0.8630\n",
      "Epoch 482/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2917 - acc: 0.8806 - val_loss: 0.3061 - val_acc: 0.8630\n",
      "Epoch 483/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2914 - acc: 0.8806 - val_loss: 0.3054 - val_acc: 0.8584\n",
      "Epoch 484/700\n",
      "1022/1022 [==============================] - 0s 39us/step - loss: 0.2912 - acc: 0.8806 - val_loss: 0.3053 - val_acc: 0.8584\n",
      "Epoch 485/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2911 - acc: 0.8816 - val_loss: 0.3049 - val_acc: 0.8584\n",
      "Epoch 486/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.2910 - acc: 0.8796 - val_loss: 0.3047 - val_acc: 0.8584\n",
      "Epoch 487/700\n",
      "1022/1022 [==============================] - 0s 41us/step - loss: 0.2909 - acc: 0.8816 - val_loss: 0.3046 - val_acc: 0.8584\n",
      "Epoch 488/700\n",
      "1022/1022 [==============================] - 0s 35us/step - loss: 0.2907 - acc: 0.8816 - val_loss: 0.3052 - val_acc: 0.8630\n",
      "Epoch 489/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2908 - acc: 0.8816 - val_loss: 0.3039 - val_acc: 0.8676\n",
      "Epoch 490/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2905 - acc: 0.8787 - val_loss: 0.3055 - val_acc: 0.8630\n",
      "Epoch 491/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2905 - acc: 0.8836 - val_loss: 0.3044 - val_acc: 0.8630\n",
      "Epoch 492/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2906 - acc: 0.8806 - val_loss: 0.3036 - val_acc: 0.8721\n",
      "Epoch 493/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.2901 - acc: 0.8796 - val_loss: 0.3032 - val_acc: 0.8767\n",
      "Epoch 494/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2905 - acc: 0.8796 - val_loss: 0.3031 - val_acc: 0.8813\n",
      "Epoch 495/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2901 - acc: 0.8806 - val_loss: 0.3029 - val_acc: 0.8813\n",
      "Epoch 496/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2903 - acc: 0.8816 - val_loss: 0.3028 - val_acc: 0.8767\n",
      "Epoch 497/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2899 - acc: 0.8777 - val_loss: 0.3026 - val_acc: 0.8813\n",
      "Epoch 498/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2897 - acc: 0.8806 - val_loss: 0.3024 - val_acc: 0.8813\n",
      "Epoch 499/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2895 - acc: 0.8796 - val_loss: 0.3027 - val_acc: 0.8630\n",
      "Epoch 500/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2891 - acc: 0.8806 - val_loss: 0.3027 - val_acc: 0.8630\n",
      "Epoch 501/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2892 - acc: 0.8816 - val_loss: 0.3022 - val_acc: 0.8767\n",
      "Epoch 502/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.2891 - acc: 0.8777 - val_loss: 0.3018 - val_acc: 0.8858\n",
      "Epoch 503/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2894 - acc: 0.8796 - val_loss: 0.3018 - val_acc: 0.8813\n",
      "Epoch 504/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2897 - acc: 0.8806 - val_loss: 0.3018 - val_acc: 0.8813\n",
      "Epoch 505/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2886 - acc: 0.8777 - val_loss: 0.3020 - val_acc: 0.8630\n",
      "Epoch 506/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2884 - acc: 0.8806 - val_loss: 0.3031 - val_acc: 0.8630\n",
      "Epoch 507/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2885 - acc: 0.8826 - val_loss: 0.3017 - val_acc: 0.8676\n",
      "Epoch 508/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2883 - acc: 0.8796 - val_loss: 0.3017 - val_acc: 0.8584\n",
      "Epoch 509/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2882 - acc: 0.8816 - val_loss: 0.3013 - val_acc: 0.8721\n",
      "Epoch 510/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2881 - acc: 0.8796 - val_loss: 0.3035 - val_acc: 0.8630\n",
      "Epoch 511/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2884 - acc: 0.8845 - val_loss: 0.3015 - val_acc: 0.8630\n",
      "Epoch 512/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2880 - acc: 0.8806 - val_loss: 0.3007 - val_acc: 0.8813\n",
      "Epoch 513/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2876 - acc: 0.8787 - val_loss: 0.3008 - val_acc: 0.8767\n",
      "Epoch 514/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2877 - acc: 0.8796 - val_loss: 0.3004 - val_acc: 0.8813\n",
      "Epoch 515/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2878 - acc: 0.8777 - val_loss: 0.3000 - val_acc: 0.8858\n",
      "Epoch 516/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2876 - acc: 0.8787 - val_loss: 0.3001 - val_acc: 0.8813\n",
      "Epoch 517/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2874 - acc: 0.8796 - val_loss: 0.2999 - val_acc: 0.8858\n",
      "Epoch 518/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2872 - acc: 0.8796 - val_loss: 0.3004 - val_acc: 0.8676\n",
      "Epoch 519/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2870 - acc: 0.8806 - val_loss: 0.2996 - val_acc: 0.8858\n",
      "Epoch 520/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2872 - acc: 0.8796 - val_loss: 0.2998 - val_acc: 0.8813\n",
      "Epoch 521/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2870 - acc: 0.8816 - val_loss: 0.2995 - val_acc: 0.8858\n",
      "Epoch 522/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2875 - acc: 0.8796 - val_loss: 0.2993 - val_acc: 0.8858\n",
      "Epoch 523/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2868 - acc: 0.8787 - val_loss: 0.2993 - val_acc: 0.8813\n",
      "Epoch 524/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2864 - acc: 0.8796 - val_loss: 0.2992 - val_acc: 0.8813\n",
      "Epoch 525/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2865 - acc: 0.8787 - val_loss: 0.3002 - val_acc: 0.8676\n",
      "Epoch 526/700\n",
      "1022/1022 [==============================] - 0s 45us/step - loss: 0.2863 - acc: 0.8826 - val_loss: 0.2991 - val_acc: 0.8813\n",
      "Epoch 527/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2862 - acc: 0.8816 - val_loss: 0.2986 - val_acc: 0.8858\n",
      "Epoch 528/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2862 - acc: 0.8806 - val_loss: 0.2992 - val_acc: 0.8676\n",
      "Epoch 529/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.2861 - acc: 0.8816 - val_loss: 0.3017 - val_acc: 0.8630\n",
      "Epoch 530/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 19us/step - loss: 0.2864 - acc: 0.8826 - val_loss: 0.2991 - val_acc: 0.8721\n",
      "Epoch 531/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2858 - acc: 0.8816 - val_loss: 0.2989 - val_acc: 0.8721\n",
      "Epoch 532/700\n",
      "1022/1022 [==============================] - 0s 20us/step - loss: 0.2855 - acc: 0.8816 - val_loss: 0.2993 - val_acc: 0.8676\n",
      "Epoch 533/700\n",
      "1022/1022 [==============================] - 0s 18us/step - loss: 0.2855 - acc: 0.8816 - val_loss: 0.2977 - val_acc: 0.8858\n",
      "Epoch 534/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2855 - acc: 0.8816 - val_loss: 0.2983 - val_acc: 0.8767\n",
      "Epoch 535/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2852 - acc: 0.8816 - val_loss: 0.2983 - val_acc: 0.8767\n",
      "Epoch 536/700\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.2620 - acc: 0.880 - 0s 23us/step - loss: 0.2851 - acc: 0.8806 - val_loss: 0.2995 - val_acc: 0.8630\n",
      "Epoch 537/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2853 - acc: 0.8816 - val_loss: 0.2971 - val_acc: 0.8858\n",
      "Epoch 538/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2853 - acc: 0.8816 - val_loss: 0.2971 - val_acc: 0.8904\n",
      "Epoch 539/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2860 - acc: 0.8806 - val_loss: 0.2969 - val_acc: 0.8904\n",
      "Epoch 540/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2859 - acc: 0.8836 - val_loss: 0.2971 - val_acc: 0.8858\n",
      "Epoch 541/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2846 - acc: 0.8826 - val_loss: 0.2967 - val_acc: 0.8858\n",
      "Epoch 542/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2845 - acc: 0.8826 - val_loss: 0.2966 - val_acc: 0.8858\n",
      "Epoch 543/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.2852 - acc: 0.8836 - val_loss: 0.2964 - val_acc: 0.8858\n",
      "Epoch 544/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.2848 - acc: 0.8836 - val_loss: 0.2974 - val_acc: 0.8767\n",
      "Epoch 545/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2843 - acc: 0.8806 - val_loss: 0.2963 - val_acc: 0.8858\n",
      "Epoch 546/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2844 - acc: 0.8816 - val_loss: 0.2969 - val_acc: 0.8813\n",
      "Epoch 547/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2840 - acc: 0.8826 - val_loss: 0.2986 - val_acc: 0.8676\n",
      "Epoch 548/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.2843 - acc: 0.8816 - val_loss: 0.2969 - val_acc: 0.8767\n",
      "Epoch 549/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2838 - acc: 0.8816 - val_loss: 0.2971 - val_acc: 0.8767\n",
      "Epoch 550/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2839 - acc: 0.8816 - val_loss: 0.2957 - val_acc: 0.8858\n",
      "Epoch 551/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2835 - acc: 0.8826 - val_loss: 0.2959 - val_acc: 0.8858\n",
      "Epoch 552/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2834 - acc: 0.8826 - val_loss: 0.2957 - val_acc: 0.8858\n",
      "Epoch 553/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2834 - acc: 0.8826 - val_loss: 0.2970 - val_acc: 0.8721\n",
      "Epoch 554/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2835 - acc: 0.8816 - val_loss: 0.2961 - val_acc: 0.8813\n",
      "Epoch 555/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2832 - acc: 0.8826 - val_loss: 0.2965 - val_acc: 0.8767\n",
      "Epoch 556/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2833 - acc: 0.8836 - val_loss: 0.2953 - val_acc: 0.8858\n",
      "Epoch 557/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2831 - acc: 0.8826 - val_loss: 0.2969 - val_acc: 0.8767\n",
      "Epoch 558/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2832 - acc: 0.8826 - val_loss: 0.2966 - val_acc: 0.8767\n",
      "Epoch 559/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2831 - acc: 0.8816 - val_loss: 0.2986 - val_acc: 0.8630\n",
      "Epoch 560/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2839 - acc: 0.8826 - val_loss: 0.2971 - val_acc: 0.8676\n",
      "Epoch 561/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2832 - acc: 0.8806 - val_loss: 0.2948 - val_acc: 0.8858\n",
      "Epoch 562/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2825 - acc: 0.8826 - val_loss: 0.2944 - val_acc: 0.8858\n",
      "Epoch 563/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2824 - acc: 0.8845 - val_loss: 0.2939 - val_acc: 0.8858\n",
      "Epoch 564/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2823 - acc: 0.8855 - val_loss: 0.2952 - val_acc: 0.8813\n",
      "Epoch 565/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2824 - acc: 0.8826 - val_loss: 0.2945 - val_acc: 0.8858\n",
      "Epoch 566/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2821 - acc: 0.8826 - val_loss: 0.2944 - val_acc: 0.8858\n",
      "Epoch 567/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2820 - acc: 0.8826 - val_loss: 0.2953 - val_acc: 0.8813\n",
      "Epoch 568/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.2823 - acc: 0.8826 - val_loss: 0.2956 - val_acc: 0.8767\n",
      "Epoch 569/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2822 - acc: 0.8806 - val_loss: 0.2932 - val_acc: 0.8858\n",
      "Epoch 570/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2818 - acc: 0.8855 - val_loss: 0.2940 - val_acc: 0.8858\n",
      "Epoch 571/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2819 - acc: 0.8826 - val_loss: 0.2935 - val_acc: 0.8858\n",
      "Epoch 572/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2816 - acc: 0.8845 - val_loss: 0.2941 - val_acc: 0.8858\n",
      "Epoch 573/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2817 - acc: 0.8836 - val_loss: 0.2931 - val_acc: 0.8858\n",
      "Epoch 574/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2817 - acc: 0.8845 - val_loss: 0.2926 - val_acc: 0.8904\n",
      "Epoch 575/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2820 - acc: 0.8855 - val_loss: 0.2927 - val_acc: 0.8904\n",
      "Epoch 576/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2823 - acc: 0.8885 - val_loss: 0.2925 - val_acc: 0.8858\n",
      "Epoch 577/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2813 - acc: 0.8855 - val_loss: 0.2922 - val_acc: 0.8904\n",
      "Epoch 578/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2816 - acc: 0.8845 - val_loss: 0.2923 - val_acc: 0.8858\n",
      "Epoch 579/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2812 - acc: 0.8845 - val_loss: 0.2920 - val_acc: 0.8858\n",
      "Epoch 580/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2814 - acc: 0.8875 - val_loss: 0.2919 - val_acc: 0.8904\n",
      "Epoch 581/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2813 - acc: 0.8894 - val_loss: 0.2918 - val_acc: 0.8858\n",
      "Epoch 582/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2811 - acc: 0.8855 - val_loss: 0.2917 - val_acc: 0.8904\n",
      "Epoch 583/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2809 - acc: 0.8855 - val_loss: 0.2918 - val_acc: 0.8858\n",
      "Epoch 584/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2810 - acc: 0.8845 - val_loss: 0.2918 - val_acc: 0.8858\n",
      "Epoch 585/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2804 - acc: 0.8845 - val_loss: 0.2919 - val_acc: 0.8858\n",
      "Epoch 586/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2805 - acc: 0.8845 - val_loss: 0.2918 - val_acc: 0.8858\n",
      "Epoch 587/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2803 - acc: 0.8855 - val_loss: 0.2920 - val_acc: 0.8858\n",
      "Epoch 588/700\n",
      "1022/1022 [==============================] - 0s 38us/step - loss: 0.2801 - acc: 0.8845 - val_loss: 0.2911 - val_acc: 0.8904\n",
      "Epoch 589/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2806 - acc: 0.8865 - val_loss: 0.2910 - val_acc: 0.8904\n",
      "Epoch 590/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.2805 - acc: 0.8885 - val_loss: 0.2908 - val_acc: 0.8904\n",
      "Epoch 591/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.2807 - acc: 0.8885 - val_loss: 0.2907 - val_acc: 0.8904\n",
      "Epoch 592/700\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2804 - acc: 0.8865 - val_loss: 0.2911 - val_acc: 0.8858\n",
      "Epoch 593/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2811 - acc: 0.8875 - val_loss: 0.2910 - val_acc: 0.8858\n",
      "Epoch 594/700\n",
      "1022/1022 [==============================] - 0s 40us/step - loss: 0.2796 - acc: 0.8855 - val_loss: 0.2912 - val_acc: 0.8858\n",
      "Epoch 595/700\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2796 - acc: 0.8855 - val_loss: 0.2903 - val_acc: 0.8904\n",
      "Epoch 596/700\n",
      "1022/1022 [==============================] - 0s 45us/step - loss: 0.2799 - acc: 0.8885 - val_loss: 0.2915 - val_acc: 0.8858\n",
      "Epoch 597/700\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.2794 - acc: 0.8855 - val_loss: 0.2903 - val_acc: 0.8858\n",
      "Epoch 598/700\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.2794 - acc: 0.8855 - val_loss: 0.2898 - val_acc: 0.8904\n",
      "Epoch 599/700\n",
      "1022/1022 [==============================] - 0s 34us/step - loss: 0.2797 - acc: 0.8875 - val_loss: 0.2900 - val_acc: 0.8904\n",
      "Epoch 600/700\n",
      "1022/1022 [==============================] - 0s 37us/step - loss: 0.2804 - acc: 0.8885 - val_loss: 0.2897 - val_acc: 0.8904\n",
      "Epoch 601/700\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.2800 - acc: 0.8894 - val_loss: 0.2895 - val_acc: 0.8904\n",
      "Epoch 602/700\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.3012 - acc: 0.870 - 0s 34us/step - loss: 0.2793 - acc: 0.8875 - val_loss: 0.2895 - val_acc: 0.8904\n",
      "Epoch 603/700\n",
      "1022/1022 [==============================] - 0s 45us/step - loss: 0.2792 - acc: 0.8885 - val_loss: 0.2905 - val_acc: 0.8858\n",
      "Epoch 604/700\n",
      "1022/1022 [==============================] - 0s 37us/step - loss: 0.2788 - acc: 0.8855 - val_loss: 0.2914 - val_acc: 0.8813\n",
      "Epoch 605/700\n",
      "1022/1022 [==============================] - 0s 58us/step - loss: 0.2789 - acc: 0.8855 - val_loss: 0.2906 - val_acc: 0.8858\n",
      "Epoch 606/700\n",
      "1022/1022 [==============================] - 0s 58us/step - loss: 0.2787 - acc: 0.8855 - val_loss: 0.2922 - val_acc: 0.8858\n",
      "Epoch 607/700\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.2792 - acc: 0.8826 - val_loss: 0.2913 - val_acc: 0.8813\n",
      "Epoch 608/700\n",
      "1022/1022 [==============================] - 0s 40us/step - loss: 0.2787 - acc: 0.8836 - val_loss: 0.2901 - val_acc: 0.8858\n",
      "Epoch 609/700\n",
      "1022/1022 [==============================] - 0s 55us/step - loss: 0.2783 - acc: 0.8855 - val_loss: 0.2902 - val_acc: 0.8858\n",
      "Epoch 610/700\n",
      "1022/1022 [==============================] - 0s 35us/step - loss: 0.2785 - acc: 0.8855 - val_loss: 0.2895 - val_acc: 0.8858\n",
      "Epoch 611/700\n",
      "1022/1022 [==============================] - 0s 42us/step - loss: 0.2782 - acc: 0.8855 - val_loss: 0.2901 - val_acc: 0.8858\n",
      "Epoch 612/700\n",
      "1022/1022 [==============================] - 0s 44us/step - loss: 0.2783 - acc: 0.8845 - val_loss: 0.2888 - val_acc: 0.8858\n",
      "Epoch 613/700\n",
      "1022/1022 [==============================] - 0s 41us/step - loss: 0.2780 - acc: 0.8865 - val_loss: 0.2899 - val_acc: 0.8858\n",
      "Epoch 614/700\n",
      "1022/1022 [==============================] - 0s 55us/step - loss: 0.2781 - acc: 0.8865 - val_loss: 0.2885 - val_acc: 0.8858\n",
      "Epoch 615/700\n",
      "1022/1022 [==============================] - 0s 35us/step - loss: 0.2784 - acc: 0.8885 - val_loss: 0.2881 - val_acc: 0.8904\n",
      "Epoch 616/700\n",
      "1022/1022 [==============================] - 0s 40us/step - loss: 0.2780 - acc: 0.8894 - val_loss: 0.2888 - val_acc: 0.8858\n",
      "Epoch 617/700\n",
      "1022/1022 [==============================] - 0s 52us/step - loss: 0.2778 - acc: 0.8865 - val_loss: 0.2889 - val_acc: 0.8858\n",
      "Epoch 618/700\n",
      "1022/1022 [==============================] - 0s 35us/step - loss: 0.2776 - acc: 0.8865 - val_loss: 0.2912 - val_acc: 0.8858\n",
      "Epoch 619/700\n",
      "1022/1022 [==============================] - 0s 45us/step - loss: 0.2780 - acc: 0.8806 - val_loss: 0.2886 - val_acc: 0.8858\n",
      "Epoch 620/700\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.2775 - acc: 0.8865 - val_loss: 0.2879 - val_acc: 0.8858\n",
      "Epoch 621/700\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.2777 - acc: 0.8865 - val_loss: 0.2874 - val_acc: 0.8904\n",
      "Epoch 622/700\n",
      "1022/1022 [==============================] - 0s 39us/step - loss: 0.2781 - acc: 0.8904 - val_loss: 0.2878 - val_acc: 0.8858\n",
      "Epoch 623/700\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2773 - acc: 0.8875 - val_loss: 0.2887 - val_acc: 0.8858\n",
      "Epoch 624/700\n",
      "1022/1022 [==============================] - 0s 59us/step - loss: 0.2773 - acc: 0.8865 - val_loss: 0.2879 - val_acc: 0.8858\n",
      "Epoch 625/700\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2771 - acc: 0.8875 - val_loss: 0.2879 - val_acc: 0.8858\n",
      "Epoch 626/700\n",
      "1022/1022 [==============================] - 0s 37us/step - loss: 0.2770 - acc: 0.8885 - val_loss: 0.2893 - val_acc: 0.8813\n",
      "Epoch 627/700\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2772 - acc: 0.8865 - val_loss: 0.2877 - val_acc: 0.8858\n",
      "Epoch 628/700\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.2770 - acc: 0.8865 - val_loss: 0.2879 - val_acc: 0.8858\n",
      "Epoch 629/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2767 - acc: 0.8865 - val_loss: 0.2872 - val_acc: 0.8858\n",
      "Epoch 630/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2767 - acc: 0.8875 - val_loss: 0.2874 - val_acc: 0.8858\n",
      "Epoch 631/700\n",
      "1022/1022 [==============================] - 0s 40us/step - loss: 0.2765 - acc: 0.8875 - val_loss: 0.2894 - val_acc: 0.8813\n",
      "Epoch 632/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2775 - acc: 0.8855 - val_loss: 0.2880 - val_acc: 0.8858\n",
      "Epoch 633/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2765 - acc: 0.8855 - val_loss: 0.2870 - val_acc: 0.8858\n",
      "Epoch 634/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2764 - acc: 0.8865 - val_loss: 0.2863 - val_acc: 0.8904\n",
      "Epoch 635/700\n",
      "1022/1022 [==============================] - 0s 39us/step - loss: 0.2766 - acc: 0.8904 - val_loss: 0.2876 - val_acc: 0.8858\n",
      "Epoch 636/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.2764 - acc: 0.8865 - val_loss: 0.2871 - val_acc: 0.8858\n",
      "Epoch 637/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2762 - acc: 0.8875 - val_loss: 0.2885 - val_acc: 0.8813\n",
      "Epoch 638/700\n",
      "1022/1022 [==============================] - 0s 33us/step - loss: 0.2769 - acc: 0.8845 - val_loss: 0.2877 - val_acc: 0.8813\n",
      "Epoch 639/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2762 - acc: 0.8855 - val_loss: 0.2872 - val_acc: 0.8858\n",
      "Epoch 640/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2760 - acc: 0.8865 - val_loss: 0.2871 - val_acc: 0.8858\n",
      "Epoch 641/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2760 - acc: 0.8865 - val_loss: 0.2870 - val_acc: 0.8858\n",
      "Epoch 642/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2760 - acc: 0.8865 - val_loss: 0.2875 - val_acc: 0.8813\n",
      "Epoch 643/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2759 - acc: 0.8865 - val_loss: 0.2859 - val_acc: 0.8904\n",
      "Epoch 644/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2761 - acc: 0.8875 - val_loss: 0.2871 - val_acc: 0.8858\n",
      "Epoch 645/700\n",
      "1022/1022 [==============================] - 0s 41us/step - loss: 0.2759 - acc: 0.8855 - val_loss: 0.2861 - val_acc: 0.8858\n",
      "Epoch 646/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2756 - acc: 0.8875 - val_loss: 0.2861 - val_acc: 0.8858\n",
      "Epoch 647/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2757 - acc: 0.8885 - val_loss: 0.2858 - val_acc: 0.8858\n",
      "Epoch 648/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 35us/step - loss: 0.2756 - acc: 0.8885 - val_loss: 0.2860 - val_acc: 0.8858\n",
      "Epoch 649/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2753 - acc: 0.8865 - val_loss: 0.2849 - val_acc: 0.8904\n",
      "Epoch 650/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2756 - acc: 0.8904 - val_loss: 0.2850 - val_acc: 0.8904\n",
      "Epoch 651/700\n",
      "1022/1022 [==============================] - 0s 43us/step - loss: 0.2755 - acc: 0.8894 - val_loss: 0.2850 - val_acc: 0.8904\n",
      "Epoch 652/700\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.2851 - acc: 0.875 - 0s 34us/step - loss: 0.2756 - acc: 0.8894 - val_loss: 0.2848 - val_acc: 0.8904\n",
      "Epoch 653/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2752 - acc: 0.8894 - val_loss: 0.2848 - val_acc: 0.8904\n",
      "Epoch 654/700\n",
      "1022/1022 [==============================] - ETA: 0s - loss: 0.2206 - acc: 0.925 - 0s 29us/step - loss: 0.2750 - acc: 0.8904 - val_loss: 0.2856 - val_acc: 0.8858\n",
      "Epoch 655/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2752 - acc: 0.8865 - val_loss: 0.2860 - val_acc: 0.8858\n",
      "Epoch 656/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2752 - acc: 0.8845 - val_loss: 0.2855 - val_acc: 0.8858\n",
      "Epoch 657/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2753 - acc: 0.8865 - val_loss: 0.2844 - val_acc: 0.8904\n",
      "Epoch 658/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2751 - acc: 0.8894 - val_loss: 0.2842 - val_acc: 0.8904\n",
      "Epoch 659/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2749 - acc: 0.8914 - val_loss: 0.2841 - val_acc: 0.8904\n",
      "Epoch 660/700\n",
      "1022/1022 [==============================] - 0s 32us/step - loss: 0.2748 - acc: 0.8904 - val_loss: 0.2840 - val_acc: 0.8904\n",
      "Epoch 661/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2750 - acc: 0.8904 - val_loss: 0.2855 - val_acc: 0.8858\n",
      "Epoch 662/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2746 - acc: 0.8865 - val_loss: 0.2845 - val_acc: 0.8904\n",
      "Epoch 663/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2744 - acc: 0.8894 - val_loss: 0.2848 - val_acc: 0.8858\n",
      "Epoch 664/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2744 - acc: 0.8855 - val_loss: 0.2840 - val_acc: 0.8904\n",
      "Epoch 665/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2745 - acc: 0.8904 - val_loss: 0.2848 - val_acc: 0.8858\n",
      "Epoch 666/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2744 - acc: 0.8875 - val_loss: 0.2836 - val_acc: 0.8858\n",
      "Epoch 667/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2749 - acc: 0.8904 - val_loss: 0.2848 - val_acc: 0.8858\n",
      "Epoch 668/700\n",
      "1022/1022 [==============================] - 0s 29us/step - loss: 0.2743 - acc: 0.8865 - val_loss: 0.2860 - val_acc: 0.8813\n",
      "Epoch 669/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2745 - acc: 0.8836 - val_loss: 0.2835 - val_acc: 0.8904\n",
      "Epoch 670/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2741 - acc: 0.8914 - val_loss: 0.2839 - val_acc: 0.8904\n",
      "Epoch 671/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2740 - acc: 0.8875 - val_loss: 0.2837 - val_acc: 0.8904\n",
      "Epoch 672/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2744 - acc: 0.8885 - val_loss: 0.2845 - val_acc: 0.8858\n",
      "Epoch 673/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2742 - acc: 0.8865 - val_loss: 0.2849 - val_acc: 0.8858\n",
      "Epoch 674/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2743 - acc: 0.8836 - val_loss: 0.2830 - val_acc: 0.8904\n",
      "Epoch 675/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2741 - acc: 0.8904 - val_loss: 0.2829 - val_acc: 0.8858\n",
      "Epoch 676/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2739 - acc: 0.8924 - val_loss: 0.2830 - val_acc: 0.8904\n",
      "Epoch 677/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2741 - acc: 0.8894 - val_loss: 0.2831 - val_acc: 0.8904\n",
      "Epoch 678/700\n",
      "1022/1022 [==============================] - 0s 31us/step - loss: 0.2736 - acc: 0.8894 - val_loss: 0.2827 - val_acc: 0.8858\n",
      "Epoch 679/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2740 - acc: 0.8904 - val_loss: 0.2829 - val_acc: 0.8904\n",
      "Epoch 680/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2738 - acc: 0.8904 - val_loss: 0.2829 - val_acc: 0.8904\n",
      "Epoch 681/700\n",
      "1022/1022 [==============================] - 0s 28us/step - loss: 0.2735 - acc: 0.8904 - val_loss: 0.2832 - val_acc: 0.8904\n",
      "Epoch 682/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2733 - acc: 0.8855 - val_loss: 0.2827 - val_acc: 0.8904\n",
      "Epoch 683/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2734 - acc: 0.8904 - val_loss: 0.2835 - val_acc: 0.8858\n",
      "Epoch 684/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2732 - acc: 0.8845 - val_loss: 0.2823 - val_acc: 0.8858\n",
      "Epoch 685/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2734 - acc: 0.8914 - val_loss: 0.2824 - val_acc: 0.8904\n",
      "Epoch 686/700\n",
      "1022/1022 [==============================] - 0s 30us/step - loss: 0.2734 - acc: 0.8904 - val_loss: 0.2832 - val_acc: 0.8858\n",
      "Epoch 687/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2730 - acc: 0.8855 - val_loss: 0.2822 - val_acc: 0.8858\n",
      "Epoch 688/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2731 - acc: 0.8894 - val_loss: 0.2821 - val_acc: 0.8904\n",
      "Epoch 689/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2732 - acc: 0.8914 - val_loss: 0.2820 - val_acc: 0.8858\n",
      "Epoch 690/700\n",
      "1022/1022 [==============================] - 0s 27us/step - loss: 0.2731 - acc: 0.8914 - val_loss: 0.2824 - val_acc: 0.8904\n",
      "Epoch 691/700\n",
      "1022/1022 [==============================] - 0s 26us/step - loss: 0.2727 - acc: 0.8894 - val_loss: 0.2828 - val_acc: 0.8904\n",
      "Epoch 692/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2729 - acc: 0.8875 - val_loss: 0.2822 - val_acc: 0.8904\n",
      "Epoch 693/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2727 - acc: 0.8894 - val_loss: 0.2822 - val_acc: 0.8904\n",
      "Epoch 694/700\n",
      "1022/1022 [==============================] - 0s 24us/step - loss: 0.2728 - acc: 0.8894 - val_loss: 0.2827 - val_acc: 0.8858\n",
      "Epoch 695/700\n",
      "1022/1022 [==============================] - 0s 23us/step - loss: 0.2728 - acc: 0.8875 - val_loss: 0.2822 - val_acc: 0.8904\n",
      "Epoch 696/700\n",
      "1022/1022 [==============================] - 0s 21us/step - loss: 0.2724 - acc: 0.8885 - val_loss: 0.2841 - val_acc: 0.8813\n",
      "Epoch 697/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2728 - acc: 0.8826 - val_loss: 0.2834 - val_acc: 0.8858\n",
      "Epoch 698/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2726 - acc: 0.8836 - val_loss: 0.2823 - val_acc: 0.8904\n",
      "Epoch 699/700\n",
      "1022/1022 [==============================] - 0s 22us/step - loss: 0.2726 - acc: 0.8865 - val_loss: 0.2832 - val_acc: 0.8858\n",
      "Epoch 700/700\n",
      "1022/1022 [==============================] - 0s 25us/step - loss: 0.2724 - acc: 0.8836 - val_loss: 0.2821 - val_acc: 0.8904\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=200, epochs=700,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 64us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.899543381717107"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
